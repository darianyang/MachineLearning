{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cb66bd5d-b494-4ada-b0e7-30efeded390f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torchvision import models, transforms\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pathlib, torch, tqdm\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77af5f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rootdir = pathlib.Path(os.path.dirname(os.path.abspath('__file__')))\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d5d0f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_images_labels(path: str, subset: str):\n",
    "    \"\"\"\"return a list of image paths and a list of image labels\n",
    "    subset: either trian, test or hidden_test\"\"\"\n",
    "    images, labels = [], []\n",
    "    with open(path, 'r') as f:\n",
    "        for l in f.readlines():\n",
    "            img_name, label = l.split(' ')\n",
    "            images.append(rootdir/subset/img_name)\n",
    "            labels.append(int(label))\n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "\n",
    "#Then let's make a dataloader for the Resnet18 model\n",
    "#the transformation below is the special requirement of Resnet18 model\n",
    "t = transforms.Compose([\n",
    "        Image.open,\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "])\n",
    "\n",
    "class vgg_dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, x, y): \n",
    "        super(vgg_dataset, self).__init__()\n",
    "        self.x = [t(img_path) for img_path in x]\n",
    "        self.x = torch.stack(self.x)\n",
    "        self.y = y\n",
    "        assert self.x.shape[0] == self.y.shape[0], \"the input x and y have different size!\"\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "    def __len__(self):\n",
    "        return self.x.shape[0]\n",
    "\n",
    "#Then build a wrapper to make it iterable\n",
    "class vgg_dataloader(torch.utils.data.DataLoader):\n",
    "    \n",
    "    def __init__(self, x, y, batch_size=256, shuffle=False):\n",
    "        super(vgg_dataloader, self).__init__(dataset=vgg_dataset(x,y), batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "\n",
    "#make a compare function to extract real keys and predictions\n",
    "#then you can compare them by different ways\n",
    "def compare(model, dataloader, device):\n",
    "    torch.cuda.empty_cache()\n",
    "    ret = np.array([0])\n",
    "    keys = np.array([0])\n",
    "    for block in tqdm.tqdm(dataloader):\n",
    "        image, key = block\n",
    "        image = image.to(device)\n",
    "        key = key.to(device)\n",
    "        \n",
    "        prediction = model(image)\n",
    "        _,prediction = torch.max(prediction,dim=1)\n",
    "        ret = np.concatenate((ret, np.array(prediction.cpu())))\n",
    "        keys = np.concatenate((keys, np.array(key.cpu())))\n",
    "    \n",
    "    return ret[1:], keys[1:]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a86e117b",
   "metadata": {},
   "source": [
    "# Load the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42c20a9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=2, bias=False)\n",
      "    (1): Softmax(dim=1)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = models.resnet18(pretrained=True)\n",
    "model.fc = torch.nn.Sequential(torch.nn.Linear(in_features=512, out_features=2, bias=False),\n",
    "                               torch.nn.Softmax(dim=1)\n",
    "                              )\n",
    "model.to(device)\n",
    "\n",
    "model.load_state_dict(torch.load('resnet_best.pt', map_location=device))\n",
    "model.eval()\n",
    "print(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "df180320",
   "metadata": {},
   "source": [
    "# Test set prerformance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da1844d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2756 2756\n"
     ]
    }
   ],
   "source": [
    "test_path = rootdir/'labels'/'test_labels.txt'\n",
    "test_images, test_labels = prepare_images_labels(test_path, 'test')\n",
    "print(len(test_images), len(test_labels))\n",
    "\n",
    "test_loader = vgg_dataloader(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b5d8e6e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [04:15<00:00, 23.19s/it]\n"
     ]
    }
   ],
   "source": [
    "predictions, targets = compare(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f9843fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9250089702188732\n"
     ]
    }
   ],
   "source": [
    "f1 = f1_score(targets, predictions)\n",
    "print(f1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1c4ca199",
   "metadata": {},
   "source": [
    "# Hidden test set predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e57d04ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_name</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C164P125ThinF_IMG_20151116_113954_cell_58.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C82P43ThinF_IMG_20150817_123824_cell_201.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C133P94ThinF_IMG_20151004_155144_cell_125.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C152P113ThinF_IMG_20151115_125038_cell_192.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C59P20thinF_IMG_20150803_113051_cell_153.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         img_name  label\n",
       "0   C164P125ThinF_IMG_20151116_113954_cell_58.png      0\n",
       "1    C82P43ThinF_IMG_20150817_123824_cell_201.png      0\n",
       "2   C133P94ThinF_IMG_20151004_155144_cell_125.png      0\n",
       "3  C152P113ThinF_IMG_20151115_125038_cell_192.png      0\n",
       "4    C59P20thinF_IMG_20150803_113051_cell_153.png      0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_path = rootdir/'labels'/'sample_submission.csv'\n",
    "df = pd.read_csv(test_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0a636ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2756/2756 [04:29<00:00, 10.24it/s]\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "for i in tqdm.tqdm(range(len(df))):\n",
    "    img_basename = df.iloc[i, 0]\n",
    "    label = df.iloc[i, 1]\n",
    "\n",
    "    img_path = rootdir/'hidden_test'/img_basename\n",
    "    x = t(img_path).to(device).unsqueeze(0)\n",
    "    prediction = torch.argmax(model(x))\n",
    "    predictions.append(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "944aec4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_name</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C164P125ThinF_IMG_20151116_113954_cell_58.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C82P43ThinF_IMG_20150817_123824_cell_201.png</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C133P94ThinF_IMG_20151004_155144_cell_125.png</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C152P113ThinF_IMG_20151115_125038_cell_192.png</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C59P20thinF_IMG_20150803_113051_cell_153.png</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         img_name  label\n",
       "0   C164P125ThinF_IMG_20151116_113954_cell_58.png      0\n",
       "1    C82P43ThinF_IMG_20150817_123824_cell_201.png      1\n",
       "2   C133P94ThinF_IMG_20151004_155144_cell_125.png      1\n",
       "3  C152P113ThinF_IMG_20151115_125038_cell_192.png      1\n",
       "4    C59P20thinF_IMG_20150803_113051_cell_153.png      1"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = [val.item() for val in predictions]\n",
    "df['label'] = predictions\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d5b06e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('submission_resnet1.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "61b4062b24dfb1010f420dad5aa3bd73a4d2af47d0ec44eafec465a35a9d7239"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
